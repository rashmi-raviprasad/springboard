{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we clean and organize our data before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a helper function that will extract features from a specific group and create a separate dataframe. We will make sure to preserve the \"rotation\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df(df, group):\n",
    "    '''\n",
    "    Method for creating a new dataframe for a specified group. \n",
    "    Preserves the 'rotation' column from the original dataframe.\n",
    "    '''\n",
    "    new_df = df[['rotation', group]].droplevel(0, axis=1)\n",
    "    new_df.rename(columns={new_df.columns[0]:'rotation'}, inplace=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL to the CSV file is passed into read_csv. If we wish to use the CSV file locally, we can uncomment the last two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://vfacstaff.ltu.edu/lshamir/data/assym/p_all_full.csv'\n",
    "df = pd.read_csv(URL)\n",
    "#FILE_LOC = 'data/data.csv\n",
    "#df = pd.read_csv(FILE_LOC, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 455 columns, so we need to group them into categories to make our analysis manageable. We will use a list of column names and their respective groups in order to create a dictionary that maps column names to groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = pd.read_csv('data/col_names.csv', names=['category', 'col_name'])\n",
    "col_groups = col_names.groupby('category')['col_name'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct a new dataframe with multi-level columns corresponding to the categories specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([df[col_groups[key]] for key in col_groups.keys()], \n",
    "            axis=1, keys=col_groups.keys())\n",
    "data_df.insert(0, 'rotation', df['rotation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are checking to make sure there are no None/nan/empty string values in dataframe, and that none of the columns have a data type of 'object' (except the 'rotation' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rashmi/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "assert data_df.select_dtypes(['object']).equals(data_df[['rotation']])\n",
    "\n",
    "for category, column in list(data_df):\n",
    "    assert data_df[data_df.loc[:, (category, column)] == None].empty\n",
    "    assert data_df[data_df.loc[:, (category, column)] == np.nan].empty\n",
    "    assert data_df[data_df.loc[:, (category, column)] == ''].empty\n",
    "    if category != 'rotation':\n",
    "        assert data_df.loc[:, (category, column)].dtype != 'object'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this dataset uses a value of -9999.00 to denote missing entries. We will replace all instances of -9999.00 with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.replace(-9999.00, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create individual dataframes based off this clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = subset_df(data_df, 'coordinates')\n",
    "devaucouleurs = subset_df(data_df, 'devaucouleurs')\n",
    "exponential = subset_df(data_df, 'exponential')\n",
    "extinction = subset_df(data_df, 'extinction')\n",
    "fiber = subset_df(data_df, 'fiber')\n",
    "flags = subset_df(data_df, 'flags')\n",
    "isophotal = subset_df(data_df, 'isophotal')\n",
    "m = subset_df(data_df, 'm')\n",
    "model = subset_df(data_df, 'model')\n",
    "object_info = subset_df(data_df, 'object_info')\n",
    "petro = subset_df(data_df, 'petro')\n",
    "position = subset_df(data_df, 'position')\n",
    "prof = subset_df(data_df, 'prof')\n",
    "psf = subset_df(data_df, 'psf')\n",
    "signal = subset_df(data_df, 'signal')\n",
    "sky = subset_df(data_df, 'sky')\n",
    "stokes = subset_df(data_df, 'stokes')\n",
    "target = subset_df(data_df, 'target')\n",
    "texture = subset_df(data_df, 'texture')\n",
    "types = subset_df(data_df, 'types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now clean and ready for analysis! We can save these as csv files locally so we do not have to run this procedure every time we need to perform analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates.to_csv('data/coordinates.csv')\n",
    "devaucouleurs.to_csv('data/devaucouleurs.csv')\n",
    "exponential.to_csv('data/exponential.csv')\n",
    "extinction.to_csv('data/extinction.csv')\n",
    "fiber.to_csv('data/fiber.csv')\n",
    "flags.to_csv('data/flags.csv')\n",
    "isophotal.to_csv('data/isophotal.csv')\n",
    "m.to_csv('data/m.csv')\n",
    "model.to_csv('data/model.csv')\n",
    "object_info.to_csv('data/object_info.csv')\n",
    "petro.to_csv('data/petro.csv')\n",
    "position.to_csv('data/position.csv')\n",
    "prof.to_csv('data/prof.csv')\n",
    "psf.to_csv('data/psf.csv')\n",
    "signal.to_csv('data/signal.csv')\n",
    "sky.to_csv('data/sky.csv')\n",
    "stokes.to_csv('data/stokes.csv')\n",
    "target.to_csv('data/target.csv')\n",
    "texture.to_csv('data/texture.csv')\n",
    "types.to_csv('data/types.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
